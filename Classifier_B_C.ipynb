{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifier_B_C.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "12JZch71hoPa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "f71a690d-46fb-492f-ec44-4e89145305ea"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFLWuBTvhwLk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "24103f25-b5d3-422d-a69d-42f2bf0b4536"
      },
      "source": [
        "from keras.models import load_model\n",
        "classifier_B_model = load_model('/content/drive/My Drive/NLP/models/LSTM_GLOVE_Pos_Emotions.h5')\n",
        "classifier_C_model = load_model('/content/drive/My Drive/NLP/models/LSTM_GLOVE_Neg_Emotions.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZZbufr9iSbP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "22f8ec01-c9d6-44f0-f69a-a31fe033fd71"
      },
      "source": [
        "classifier_B_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Pre-trained Glove.6B.300d\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 280, 300)          6555000   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 32)                42624     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 6,597,690\n",
            "Trainable params: 6,597,690\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DAkizEiibk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "45640a1f-8676-4270-d463-864b4f6b7215"
      },
      "source": [
        "classifier_C_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"Pre-trained Glove.6B.300d\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 280, 300)          5148300   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 32)                42624     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 99        \n",
            "=================================================================\n",
            "Total params: 5,191,023\n",
            "Trainable params: 5,191,023\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRNqxsdCmDa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34da685e-4638-4549-be68-90140e4d6a8a"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "#K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.metrics import precision_score , recall_score\n",
        "  \n",
        "def append_emotion(input_file, pos_emotion_file, neg_emotion_file):\n",
        "  dataset = input_file\n",
        "  data = pd.read_csv(dataset) \n",
        "  data.drop(data.loc[data.sentiment != 1].index, inplace=True)\n",
        "  #prepare data\n",
        "  maxlen = 280\n",
        "  max_words = 40000\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  #tokenizer.fit_on_texts(data['Tweet'])\n",
        "  #sequences = tokenizer.texts_to_sequences(data['Tweet'])\n",
        "\n",
        "  tokenizer.fit_on_texts(data['text'])\n",
        "  sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "  data1 = pad_sequences(sequences, maxlen=maxlen)\n",
        "  y_val_pred=classifier_B_model.predict_classes(data1)\n",
        "  data['emotion'] = y_val_pred\n",
        "  data.to_csv(pos_emotion_file)\n",
        "\n",
        "  #write negative emotions\n",
        "  data = pd.read_csv(dataset) \n",
        "  data.drop(data.loc[data.sentiment != 0].index, inplace=True)\n",
        "  #prepare data\n",
        "  maxlen = 280\n",
        "  max_words = 40000\n",
        "  tokenizer = Tokenizer(num_words=max_words)\n",
        "  #tokenizer.fit_on_texts(data['Tweet'])\n",
        "  #sequences = tokenizer.texts_to_sequences(data['Tweet'])\n",
        "\n",
        "  tokenizer.fit_on_texts(data['text'])\n",
        "  sequences = tokenizer.texts_to_sequences(data['text'])\n",
        "\n",
        "  word_index = tokenizer.word_index\n",
        "  print('Found %s unique tokens.' % len(word_index))\n",
        "  data1 = pad_sequences(sequences, maxlen=maxlen)\n",
        "  y_val_pred=classifier_C_model.predict_classes(data1)\n",
        "  data['emotion'] = y_val_pred\n",
        "  data.to_csv(neg_emotion_file)\n",
        "  \n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGucZxV3ieLi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "41e6ee7c-93ef-4437-d9dd-e9f2f96196b2"
      },
      "source": [
        "input_file = '/content/drive/My Drive/NLP/COVID/Country_Sentiments/feb_dataset/Pakistan.CSV'\n",
        "pos_file = '/content/drive/My Drive/NLP/COVID/country_emotions/Pakistan_Pos_Emotions.csv'\n",
        "neg_file = '/content/drive/My Drive/NLP/COVID/country_emotions/Pakistan_Neg_Emotions.csv'\n",
        "append_emotion(input_file,pos_file,neg_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5347 unique tokens.\n",
            "Found 8175 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HJjqi6Ol3Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}